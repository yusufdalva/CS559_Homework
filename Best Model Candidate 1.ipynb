{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "introductory-savannah",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version used: 2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from data.data_utils import Dataset, save_dataset\n",
    "import os\n",
    "from model import AgeModel\n",
    "import tensorflow.keras.losses as losses\n",
    "\n",
    "# Set following config to resolve GPU errors\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "sess.as_default()\n",
    "print(\"Tensorflow version used: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "objective-filing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs used by Tensorflow: 1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "def check_if_gpu_used():\n",
    "    gpu_names = [x.name for x in device_lib.list_local_devices() if x.device_type == 'GPU']\n",
    "    if len(gpu_names) >= 1:\n",
    "        print(\"Number of GPUs used by Tensorflow: {}\".format(len(gpu_names)))\n",
    "    else:\n",
    "        print(\"Tensorflow operates on CPU now.\")\n",
    "check_if_gpu_used()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "responsible-chain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring compressed data details\n",
      "Training data shape: (5400, 91, 91, 1)\n",
      "Training labels shape: (5400,)\n",
      "Validation data shape: (2315, 91, 91, 1)\n",
      "Validation labels shape: (2315,)\n",
      "Testing data shape: (1159, 91, 91, 1)\n",
      "Testing labels shape: (1159,)\n",
      "Time to construct the dataset from compressed file: 0.181 seconds\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from time import time\n",
    "data_path = os.path.join(os.getcwd(), 'data')\n",
    "datafile_path = os.path.join(data_path, \"dataset.h5\")\n",
    "#save_dataset(data_path, dataset) # Compressing data in h5 format\n",
    "start = time()\n",
    "f = h5py.File(datafile_path, \"r\")\n",
    "train_samples = np.array(f[\"train_samples\"])\n",
    "train_labels = np.array(f[\"train_labels\"])\n",
    "val_samples = np.array(f[\"val_samples\"])\n",
    "val_labels = np.array(f[\"val_labels\"])\n",
    "test_samples = np.array(f[\"test_samples\"])\n",
    "test_labels = np.array(f[\"test_labels\"])\n",
    "end = time()\n",
    "f.close()\n",
    "print(\"Monitoring compressed data details\")\n",
    "## Training set\n",
    "print(\"Training data shape: {}\".format(train_samples.shape))\n",
    "print(\"Training labels shape: {}\".format(train_labels.shape))\n",
    "## Validation set\n",
    "print(\"Validation data shape: {}\".format(val_samples.shape))\n",
    "print(\"Validation labels shape: {}\".format(val_labels.shape))\n",
    "## Test set\n",
    "print(\"Testing data shape: {}\".format(test_samples.shape))\n",
    "print(\"Testing labels shape: {}\".format(test_labels.shape))\n",
    "print(\"Time to construct the dataset from compressed file: {:.3f} seconds\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "interested-nancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model specification\n",
    "non_regularized_model = []\n",
    "non_regularized_model.append({\"type\": \"conv2d\", \"filters\": 32, \"kernel_size\": (3,3), \"strides\":(1,1), \"padding\":\"valid\", \"activation\": \"relu\", \"initializer\": \"xavier\", \"regularizer\": None, \"reg_ratio\": None})\n",
    "non_regularized_model.append({\"type\": \"conv2d\", \"filters\": 32, \"kernel_size\": (3,3), \"strides\":(1,1), \"padding\":\"valid\", \"activation\": \"relu\", \"initializer\": \"xavier\", \"regularizer\": None,\"reg_ratio\": None})\n",
    "non_regularized_model.append({\"type\": \"pool\", \"pool_type\": \"max\", \"pool_size\": (2,2), \"strides\": None, \"padding\": \"valid\"})\n",
    "non_regularized_model.append({\"type\": \"conv2d\", \"filters\": 64, \"kernel_size\": (3,3), \"strides\":(1,1), \"padding\":\"valid\", \"activation\": \"relu\", \"initializer\": \"xavier\", \"regularizer\": None, \"reg_ratio\": None})\n",
    "non_regularized_model.append({\"type\": \"conv2d\", \"filters\": 64, \"kernel_size\": (3,3), \"strides\":(1,1), \"padding\":\"valid\", \"activation\": \"relu\", \"initializer\": \"xavier\", \"regularizer\": None, \"reg_ratio\": None})\n",
    "non_regularized_model.append({\"type\": \"pool\", \"pool_type\": \"max\", \"pool_size\": (2,2), \"strides\": None, \"padding\": \"valid\"})\n",
    "non_regularized_model.append({\"type\": \"conv2d\", \"filters\": 128, \"kernel_size\": (3,3), \"strides\":(1,1), \"padding\":\"valid\", \"activation\": \"relu\", \"initializer\": \"xavier\", \"regularizer\": None, \"reg_ratio\": None})\n",
    "non_regularized_model.append({\"type\": \"conv2d\", \"filters\": 128, \"kernel_size\": (3,3), \"strides\":(1,1), \"padding\":\"valid\", \"activation\": \"relu\", \"initializer\": \"xavier\", \"regularizer\": None, \"reg_ratio\": None})\n",
    "non_regularized_model.append({\"type\": \"conv2d\", \"filters\": 128, \"kernel_size\": (3,3), \"strides\":(1,1), \"padding\":\"valid\", \"activation\": \"relu\", \"initializer\": \"xavier\", \"regularizer\": None, \"reg_ratio\": None})\n",
    "non_regularized_model.append({\"type\": \"pool\", \"pool_type\": \"max\", \"pool_size\": (2,2), \"strides\": None, \"padding\": \"valid\"})\n",
    "non_regularized_model.append({\"type\": \"conv2d\", \"filters\": 256, \"kernel_size\": (3,3), \"strides\":(1,1), \"padding\":\"valid\", \"activation\": \"relu\", \"initializer\": \"xavier\", \"regularizer\": None, \"reg_ratio\": None})\n",
    "non_regularized_model.append({\"type\": \"conv2d\", \"filters\": 512, \"kernel_size\": (1,1), \"strides\":(1,1), \"padding\":\"valid\", \"activation\": \"relu\", \"initializer\": \"xavier\", \"regularizer\": None, \"reg_ratio\": None})\n",
    "non_regularized_model.append({\"type\": \"pool\", \"pool_type\": \"max\", \"pool_size\": (2,2), \"strides\": None, \"padding\": \"valid\"})\n",
    "non_regularized_model.append({\"type\": \"flatten\"})\n",
    "non_regularized_model.append({\"type\": \"dense\", \"units\": 256, \"activation\": \"relu\", \"initializer\": \"xavier\", \"regularizer\": None, \"reg_ratio\": None})\n",
    "non_regularized_model.append({\"type\": \"dense\", \"units\": 128, \"activation\": \"relu\", \"initializer\": \"xavier\", \"regularizer\": None, \"reg_ratio\": None})\n",
    "non_regularized_model.append({\"type\": \"dense\", \"units\": 64, \"activation\": \"relu\", \"initializer\": \"xavier\", \"regularizer\": None, \"reg_ratio\": None})\n",
    "non_regularized_model.append({\"type\": \"dense\", \"units\": 1, \"activation\": \"relu\", \"initializer\": \"xavier\", \"regularizer\": None, \"reg_ratio\": None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vertical-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_model(template_model, reg_ratio):\n",
    "    model = template_model.copy()\n",
    "    for layer in model:\n",
    "        if layer[\"type\"] in (\"conv2d\", \"dense\"):\n",
    "            layer[\"regularizer\"] = \"l2\"\n",
    "            layer[\"reg_ratio\"] = reg_ratio\n",
    "            if layer[\"type\"] == \"conv2d\":\n",
    "                layer[\"batch_norm\"] = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "exposed-group",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def visualize_experiment(history):\n",
    "    losses = history.history[\"loss\"]\n",
    "    valid_losses = history.history[\"val_loss\"]\n",
    "    print(\"Training loss for experiment: {}\".format(losses[-1]))\n",
    "    print(\"Minimum validation loss value achieved: {}\".format(np.amin(valid_losses)))\n",
    "    plt.plot(range(1, len(losses) + 1), losses, color=\"blue\", label=\"Training Loss\")\n",
    "    plt.plot(range(1, len(losses) + 1), valid_losses, color=\"orange\", label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss plot for best model candidate\")\n",
    "    plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "handy-feeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_training(layer_details, train_data, val_data, lr):\n",
    "    exp_model = AgeModel(layer_details, \"channels_last\")\n",
    "    exp_model.build_comp_graph((None, 91, 91, 1))\n",
    "    optim = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.MeanAbsoluteError()\n",
    "    exp_model.compile(loss=loss, optimizer=optim)\n",
    "    history = exp_model.fit(x=train_data[0], y=train_data[1], epochs=100, verbose=2, validation_data=val_data)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_model = get_candidate_model(non_regularized_model, 1e-5)\n",
    "init_hist = initial_training(init_model, (train_samples, train_labels), (val_samples, val_labels), 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-locator",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_experiment(init_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "complicated-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-celebration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_lr_reg(step_count, template_model, training_samples, training_labels, valid_samples, valid_labels):\n",
    "    lr_exp = np.random.uniform(-6, -3, step_count)\n",
    "    reg_exp = np.random.uniform(-4, 1, step_count)\n",
    "    random_idx = np.random.permutation(training_samples.shape[0])\n",
    "    results = []\n",
    "    # Results stored as (val_loss, train_loss, lr, reg_ratio)\n",
    "    for exp_idx in tqdm(range(step_count)):\n",
    "        model_metadata = get_candidate_model(template_model, 10**reg_exp[exp_idx])\n",
    "        exp_model = AgeModel(model_metadata, \"channels_last\")\n",
    "        exp_model.build_comp_graph((None, 91, 91, 1))\n",
    "        optim = tf.keras.optimizers.Adam(learning_rate=10**lr_exp[exp_idx])\n",
    "        loss = tf.keras.losses.MeanAbsoluteError()\n",
    "        exp_model.compile(loss=loss, optimizer=optim)\n",
    "        history = exp_model.fit(x=training_samples[random_idx[:500]], y=training_labels[random_idx[:500]], epochs=5, verbose=0, validation_data=(valid_samples, valid_labels))\n",
    "        results.append((history.history[\"val_loss\"][-1], history.history[\"loss\"][-1], lr_exp[exp_idx], reg_exp[exp_idx]))\n",
    "    results.sort(key=lambda x: x[0])\n",
    "    return results\n",
    "opt_results = optimize_lr_reg(100, non_regularized_model, train_samples, train_labels, val_samples, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best results obtained\")\n",
    "for result in opt_results[:10]:\n",
    "    print(\"Valid Loss: {:.3f}, Train Loss: {:.3f}, lr: {:.3f}, reg ratio: {:.3f}\".format(result[0], result[1], result[2], result[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "collaborative-bouquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_reg_smaller_range(step_count, template_model, training_samples, training_labels, valid_samples, valid_labels):\n",
    "    lr_exp = np.random.uniform(-5, -3, step_count)\n",
    "    reg_exp = np.random.uniform(-5, -2, step_count)\n",
    "    random_idx = np.random.permutation(training_samples.shape[0])\n",
    "    results = []\n",
    "    # Results stored as (val_loss, train_loss, lr, reg_ratio)\n",
    "    for exp_idx in tqdm(range(step_count)):\n",
    "        model_metadata = get_candidate_model(template_model, 10**reg_exp[exp_idx])\n",
    "        exp_model = AgeModel(model_metadata, \"channels_last\")\n",
    "        exp_model.build_comp_graph((None, 91, 91, 1))\n",
    "        optim = tf.keras.optimizers.Adam(learning_rate=10**lr_exp[exp_idx])\n",
    "        loss = tf.keras.losses.MeanAbsoluteError()\n",
    "        exp_model.compile(loss=loss, optimizer=optim)\n",
    "        history = exp_model.fit(x=training_samples[random_idx[:500]], y=training_labels[random_idx[:500]], epochs=5, verbose=0, validation_data=(valid_samples, valid_labels),batch_size=64)\n",
    "        results.append((history.history[\"val_loss\"][-1], history.history[\"loss\"][-1], lr_exp[exp_idx], reg_exp[exp_idx]))\n",
    "    results.sort(key=lambda x: x[0])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "looking-father",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [06:25<00:00, 12.85s/it]\n"
     ]
    }
   ],
   "source": [
    "opt_stage_2 = lr_reg_smaller_range(30, non_regularized_model, train_samples, train_labels, val_samples, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "wrapped-secret",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best results obtained\n",
      "Valid Loss: 28.818, Train Loss: 9.773, lr: -3.451, reg ratio: -4.183\n",
      "Valid Loss: 30.020, Train Loss: 11.807, lr: -3.103, reg ratio: -3.039\n",
      "Valid Loss: 30.384, Train Loss: 9.068, lr: -3.371, reg ratio: -3.851\n",
      "Valid Loss: 30.747, Train Loss: 21.833, lr: -4.596, reg ratio: -3.910\n",
      "Valid Loss: 31.172, Train Loss: 11.418, lr: -3.703, reg ratio: -2.977\n",
      "Valid Loss: 31.184, Train Loss: 9.696, lr: -3.116, reg ratio: -4.914\n",
      "Valid Loss: 31.209, Train Loss: 11.587, lr: -3.897, reg ratio: -3.718\n",
      "Valid Loss: 31.333, Train Loss: 9.506, lr: -3.706, reg ratio: -4.776\n",
      "Valid Loss: 31.602, Train Loss: 14.228, lr: -4.379, reg ratio: -4.605\n",
      "Valid Loss: 31.716, Train Loss: 10.493, lr: -3.573, reg ratio: -2.907\n"
     ]
    }
   ],
   "source": [
    "print(\"Best results obtained\")\n",
    "for result in opt_stage_2[:10]:\n",
    "    print(\"Valid Loss: {:.3f}, Train Loss: {:.3f}, lr: {:.3f}, reg ratio: {:.3f}\".format(result[0], result[1], result[2], result[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sought-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optim_training(layer_details, train_data, val_data, lr):\n",
    "    exp_model = AgeModel(layer_details, \"channels_last\")\n",
    "    exp_model.build_comp_graph((None, 91, 91, 1))\n",
    "    optim = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss = tf.keras.losses.MeanAbsoluteError()\n",
    "    exp_model.compile(loss=loss, optimizer=optim)\n",
    "    history = exp_model.fit(x=train_data[0], y=train_data[1], epochs=100, verbose=2, validation_data=val_data, batch_size=64)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "marked-therapist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0410s vs `on_train_batch_end` time: 0.0680s). Check your callbacks.\n",
      "85/85 - 11s - loss: 16.2642 - val_loss: 30.2645\n",
      "Epoch 2/100\n",
      "85/85 - 10s - loss: 11.4933 - val_loss: 28.2368\n",
      "Epoch 3/100\n",
      "85/85 - 10s - loss: 10.0091 - val_loss: 22.2598\n",
      "Epoch 4/100\n",
      "85/85 - 10s - loss: 8.8613 - val_loss: 16.0691\n",
      "Epoch 5/100\n",
      "85/85 - 10s - loss: 8.0203 - val_loss: 13.0296\n",
      "Epoch 6/100\n",
      "85/85 - 10s - loss: 7.1568 - val_loss: 11.8878\n",
      "Epoch 7/100\n",
      "85/85 - 10s - loss: 6.6746 - val_loss: 10.6753\n",
      "Epoch 8/100\n",
      "85/85 - 10s - loss: 6.2291 - val_loss: 11.3045\n",
      "Epoch 9/100\n",
      "85/85 - 10s - loss: 6.0226 - val_loss: 10.8263\n",
      "Epoch 10/100\n",
      "85/85 - 10s - loss: 5.3314 - val_loss: 10.9142\n",
      "Epoch 11/100\n",
      "85/85 - 10s - loss: 5.0928 - val_loss: 10.1271\n",
      "Epoch 12/100\n",
      "85/85 - 10s - loss: 5.0217 - val_loss: 9.3355\n",
      "Epoch 13/100\n",
      "85/85 - 10s - loss: 4.7570 - val_loss: 9.3776\n",
      "Epoch 14/100\n",
      "85/85 - 10s - loss: 4.5304 - val_loss: 9.3146\n",
      "Epoch 15/100\n",
      "85/85 - 10s - loss: 4.3140 - val_loss: 9.3017\n",
      "Epoch 16/100\n",
      "85/85 - 10s - loss: 4.3013 - val_loss: 9.2369\n",
      "Epoch 17/100\n",
      "85/85 - 10s - loss: 4.1197 - val_loss: 9.0066\n",
      "Epoch 18/100\n",
      "85/85 - 10s - loss: 4.1067 - val_loss: 9.0989\n",
      "Epoch 19/100\n",
      "85/85 - 10s - loss: 3.9639 - val_loss: 9.0675\n",
      "Epoch 20/100\n",
      "85/85 - 10s - loss: 3.9684 - val_loss: 8.9428\n",
      "Epoch 21/100\n",
      "85/85 - 10s - loss: 3.8821 - val_loss: 9.5088\n",
      "Epoch 22/100\n",
      "85/85 - 10s - loss: 3.7990 - val_loss: 8.9040\n",
      "Epoch 23/100\n",
      "85/85 - 10s - loss: 3.8296 - val_loss: 8.6078\n",
      "Epoch 24/100\n",
      "85/85 - 10s - loss: 3.6335 - val_loss: 8.7816\n",
      "Epoch 25/100\n",
      "85/85 - 10s - loss: 3.8891 - val_loss: 8.5984\n",
      "Epoch 26/100\n",
      "85/85 - 10s - loss: 3.5151 - val_loss: 8.4209\n",
      "Epoch 27/100\n",
      "85/85 - 10s - loss: 3.6601 - val_loss: 8.6724\n",
      "Epoch 28/100\n",
      "85/85 - 10s - loss: 3.3987 - val_loss: 8.5091\n",
      "Epoch 29/100\n",
      "85/85 - 10s - loss: 3.4338 - val_loss: 8.5961\n",
      "Epoch 30/100\n",
      "85/85 - 10s - loss: 3.3920 - val_loss: 8.8031\n",
      "Epoch 31/100\n",
      "85/85 - 10s - loss: 3.3481 - val_loss: 8.8747\n",
      "Epoch 32/100\n",
      "85/85 - 10s - loss: 3.4392 - val_loss: 8.5691\n",
      "Epoch 33/100\n",
      "85/85 - 10s - loss: 3.2398 - val_loss: 8.4603\n",
      "Epoch 34/100\n",
      "85/85 - 10s - loss: 3.5436 - val_loss: 8.4546\n",
      "Epoch 35/100\n",
      "85/85 - 10s - loss: 3.2786 - val_loss: 8.4331\n",
      "Epoch 36/100\n",
      "85/85 - 10s - loss: 3.2243 - val_loss: 8.9276\n",
      "Epoch 37/100\n",
      "85/85 - 10s - loss: 3.1830 - val_loss: 8.5177\n",
      "Epoch 38/100\n",
      "85/85 - 10s - loss: 3.3206 - val_loss: 8.4559\n",
      "Epoch 39/100\n",
      "85/85 - 10s - loss: 3.3101 - val_loss: 8.4047\n",
      "Epoch 40/100\n",
      "85/85 - 10s - loss: 3.3043 - val_loss: 8.3739\n",
      "Epoch 41/100\n",
      "85/85 - 10s - loss: 3.1732 - val_loss: 8.2959\n",
      "Epoch 42/100\n",
      "85/85 - 10s - loss: 3.1200 - val_loss: 8.2296\n",
      "Epoch 43/100\n",
      "85/85 - 10s - loss: 3.1692 - val_loss: 8.5722\n",
      "Epoch 44/100\n",
      "85/85 - 10s - loss: 3.1022 - val_loss: 8.2648\n",
      "Epoch 45/100\n",
      "85/85 - 10s - loss: 3.0340 - val_loss: 8.2610\n",
      "Epoch 46/100\n",
      "85/85 - 10s - loss: 3.0326 - val_loss: 8.3525\n",
      "Epoch 47/100\n",
      "85/85 - 10s - loss: 3.0687 - val_loss: 8.2197\n",
      "Epoch 48/100\n",
      "85/85 - 10s - loss: 3.0296 - val_loss: 8.3790\n",
      "Epoch 49/100\n",
      "85/85 - 10s - loss: 2.9857 - val_loss: 8.1522\n",
      "Epoch 50/100\n",
      "85/85 - 10s - loss: 2.9591 - val_loss: 8.1941\n",
      "Epoch 51/100\n",
      "85/85 - 10s - loss: 2.9688 - val_loss: 8.1625\n",
      "Epoch 52/100\n",
      "85/85 - 10s - loss: 2.9192 - val_loss: 8.1673\n",
      "Epoch 53/100\n",
      "85/85 - 10s - loss: 2.8625 - val_loss: 8.3477\n",
      "Epoch 54/100\n",
      "85/85 - 10s - loss: 2.9762 - val_loss: 8.1999\n",
      "Epoch 55/100\n",
      "85/85 - 10s - loss: 2.9781 - val_loss: 8.0305\n",
      "Epoch 56/100\n",
      "85/85 - 10s - loss: 2.9438 - val_loss: 8.0641\n",
      "Epoch 57/100\n",
      "85/85 - 10s - loss: 2.8755 - val_loss: 8.0976\n",
      "Epoch 58/100\n",
      "85/85 - 10s - loss: 2.7699 - val_loss: 8.0253\n",
      "Epoch 59/100\n",
      "85/85 - 10s - loss: 2.9640 - val_loss: 8.0738\n",
      "Epoch 60/100\n",
      "85/85 - 10s - loss: 2.8915 - val_loss: 8.3496\n",
      "Epoch 61/100\n",
      "85/85 - 10s - loss: 2.9011 - val_loss: 8.2070\n",
      "Epoch 62/100\n",
      "85/85 - 10s - loss: 2.8577 - val_loss: 8.1306\n",
      "Epoch 63/100\n",
      "85/85 - 10s - loss: 2.8686 - val_loss: 8.1550\n",
      "Epoch 64/100\n",
      "85/85 - 10s - loss: 2.7391 - val_loss: 8.2885\n",
      "Epoch 65/100\n",
      "85/85 - 10s - loss: 2.7154 - val_loss: 8.1006\n",
      "Epoch 66/100\n",
      "85/85 - 10s - loss: 2.9182 - val_loss: 8.2008\n",
      "Epoch 67/100\n",
      "85/85 - 10s - loss: 2.7698 - val_loss: 8.0789\n",
      "Epoch 68/100\n",
      "85/85 - 10s - loss: 2.7218 - val_loss: 8.0135\n",
      "Epoch 69/100\n",
      "85/85 - 10s - loss: 2.7868 - val_loss: 8.0204\n",
      "Epoch 70/100\n",
      "85/85 - 10s - loss: 2.8627 - val_loss: 8.0516\n",
      "Epoch 71/100\n",
      "85/85 - 10s - loss: 2.7512 - val_loss: 8.1450\n",
      "Epoch 72/100\n",
      "85/85 - 10s - loss: 2.7933 - val_loss: 7.9943\n",
      "Epoch 73/100\n",
      "85/85 - 10s - loss: 2.6842 - val_loss: 7.9452\n",
      "Epoch 74/100\n",
      "85/85 - 10s - loss: 2.8062 - val_loss: 8.0692\n",
      "Epoch 75/100\n",
      "85/85 - 10s - loss: 2.7463 - val_loss: 7.9510\n",
      "Epoch 76/100\n",
      "85/85 - 10s - loss: 2.7189 - val_loss: 8.0043\n",
      "Epoch 77/100\n",
      "85/85 - 10s - loss: 2.7555 - val_loss: 8.0534\n",
      "Epoch 78/100\n",
      "85/85 - 10s - loss: 2.6454 - val_loss: 8.2440\n",
      "Epoch 79/100\n",
      "85/85 - 10s - loss: 2.6271 - val_loss: 8.1337\n",
      "Epoch 80/100\n",
      "85/85 - 10s - loss: 2.5804 - val_loss: 8.2896\n",
      "Epoch 81/100\n",
      "85/85 - 10s - loss: 2.7522 - val_loss: 7.9649\n",
      "Epoch 82/100\n",
      "85/85 - 10s - loss: 2.5848 - val_loss: 7.9948\n",
      "Epoch 83/100\n",
      "85/85 - 10s - loss: 2.6563 - val_loss: 7.9507\n",
      "Epoch 84/100\n",
      "85/85 - 10s - loss: 2.6082 - val_loss: 7.9568\n",
      "Epoch 85/100\n",
      "85/85 - 10s - loss: 2.5901 - val_loss: 7.9704\n",
      "Epoch 86/100\n",
      "85/85 - 10s - loss: 2.5988 - val_loss: 7.9000\n",
      "Epoch 87/100\n",
      "85/85 - 10s - loss: 2.6373 - val_loss: 8.2982\n",
      "Epoch 88/100\n",
      "85/85 - 10s - loss: 2.6956 - val_loss: 7.9442\n",
      "Epoch 89/100\n",
      "85/85 - 10s - loss: 2.6159 - val_loss: 7.8356\n",
      "Epoch 90/100\n",
      "85/85 - 10s - loss: 2.6404 - val_loss: 7.9257\n",
      "Epoch 91/100\n",
      "85/85 - 10s - loss: 2.5942 - val_loss: 8.1669\n",
      "Epoch 92/100\n",
      "85/85 - 10s - loss: 2.6188 - val_loss: 8.3010\n",
      "Epoch 93/100\n",
      "85/85 - 10s - loss: 2.6234 - val_loss: 7.9968\n",
      "Epoch 94/100\n",
      "85/85 - 10s - loss: 2.5961 - val_loss: 7.9217\n",
      "Epoch 95/100\n",
      "85/85 - 10s - loss: 2.5722 - val_loss: 7.9412\n",
      "Epoch 96/100\n",
      "85/85 - 10s - loss: 2.5666 - val_loss: 7.7758\n",
      "Epoch 97/100\n",
      "85/85 - 10s - loss: 2.5453 - val_loss: 7.8425\n",
      "Epoch 98/100\n",
      "85/85 - 10s - loss: 2.5314 - val_loss: 7.9237\n",
      "Epoch 99/100\n",
      "85/85 - 10s - loss: 2.5482 - val_loss: 8.0317\n",
      "Epoch 100/100\n",
      "85/85 - 10s - loss: 2.5619 - val_loss: 7.8708\n"
     ]
    }
   ],
   "source": [
    "init_model = get_candidate_model(non_regularized_model, 1e-3)\n",
    "init_hist = optim_training(init_model, (train_samples, train_labels), (val_samples, val_labels), 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-locking",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
